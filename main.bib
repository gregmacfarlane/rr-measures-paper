
@book{popper2005logic,
  title={The logic of scientific discovery},
  author={Popper, Karl},
  year={2005},
  publisher={Routledge}
}

@article{ROONEY2016617,
title = {How credible are the study results? Evaluating and applying internal validity tools to literature-based assessments of environmental health hazards},
journal = {Environment International},
volume = {92-93},
pages = {617-629},
year = {2016},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0160412016300058},
author = {Andrew A. Rooney and Glinda S. Cooper and Gloria D. Jahnke and Juleen Lam and Rebecca L. Morgan and Abee L. Boyles and Jennifer M. Ratcliffe and Andrew D. Kraft and Holger J. Schünemann and Pamela Schwingl and Teneille D. Walker and Kristina A. Thayer and Ruth M. Lunn},
keywords = {Risk of bias, Internal validity, Systematic review, Environmental health, Hazard assessment},
}

@article{nosek2015promoting,
  title={Promoting an open research culture},
  author={Nosek, Brian A and Alter, George and Banks, George C and Borsboom, Denny and Bowman, Sara D and Breckler, Steven J and Buck, Stuart and Chambers, Christopher D and Chin, Gilbert and Christensen, Garret and others},
  journal={Science},
  volume={348},
  number={6242},
  pages={1422--1425},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@article{stodden2016enhancing,
  title={Enhancing reproducibility for computational methods},
  author={Stodden, Victoria and McNutt, Marcia and Bailey, David H and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A and Ioannidis, John PA and Taufer, Michela},
  journal={Science},
  volume={354},
  number={6317},
  pages={1240--1241},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@article{munafo2017manifesto,
  title={A manifesto for reproducible science},
  author={Munaf{\`o}, Marcus R and Nosek, Brian A and Bishop, Dorothy VM and Button, Katherine S and Chambers, Christopher D and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J and Ioannidis, John PA},
  journal={Nature human behaviour},
  volume={1},
  number={1},
  pages={0021},
  year={2017},
  publisher={Nature Publishing Group UK London}
}

@article{resnik2017reproducibility,
  title={Reproducibility and research integrity},
  author={Resnik, David B and Shamoo, Adil E},
  journal={Accountability in research},
  volume={24},
  number={2},
  pages={116--123},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{ZHENG2021100004,
title = {Reasons, challenges, and some tools for doing reproducible transportation research},
journal = {Communications in Transportation Research},
volume = {1},
pages = {100004},
year = {2021},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2021.100004},
url = {https://www.sciencedirect.com/science/article/pii/S2772424721000044},
author = {Zuduo Zheng},
keywords = {Reproducible research (RR), Transportation research, R, RStudio, R markdown},
abstract = {This paper introduces reproducible research (RR), and explains its importance, benefits, and challenges. Some important tools for conducting RR in Transportation Research are also introduced. Moreover, the source code for generating this paper has been designed in a way so that it can be used as a template for researchers to write their future journal papers as dynamic and reproducible documents.}
}

@article{wood2024reproducibility,
  title={Reproducibility in Transportation Research: Importance, Best Practices, and Dealing with Protected and Sensitive Data},
  author={Wood, Jonathan S and van Schalkwyk, Ida},
  journal={Journal of Transportation Technologies},
  volume={15},
  number={1},
  pages={179--202},
  year={2024},
  publisher={Scientific Research Publishing}
}


@misc{tatman2018practical,
  title={A practical taxonomy of reproducibility for machine learning research},
  author={Tatman, Rachael and VanderPlas, Jake and Dane, Sohier}
}

@ARTICLE{4815541,
  author={Vandewalle, Patrick and Kovacevic, Jelena and Vetterli, Martin},
  journal={IEEE Signal Processing Magazine}, 
  title={Reproducible research in signal processing}, 
  year={2009},
  volume={26},
  number={3},
  pages={37-47},
  keywords={Signal processing;Signal processing algorithms;Scholarships;Digital signal processing;Wikipedia;Testing;Advertising;Programming;Education;Reproducibility of results},
  doi={10.1109/MSP.2009.932122}}

@article{pineau2021improving, title={Improving reproducibility in machine learning research (a report from the neurips 2019 reproducibility program)}, author={Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv and Larivi{\`e}re, Vincent and Beygelzimer, Alina and d'Alch{\'e}-Buc, Florence and Fox, Emily and Larochelle, Hugo}, journal={Journal of machine learning research}, volume={22}, number={164}, pages={1--20}, year={2021} } 

@inproceedings{10.1145/3576915.3623130,
author = {Olszewski, Daniel and Lu, Allison and Stillman, Carson and Warren, Kevin and Kitroser, Cole and Pascual, Alejandro and Ukirde, Divyajyoti and Butler, Kevin and Traynor, Patrick},
title = {"Get in Researchers; We're Measuring Reproducibility": A Reproducibility Study of Machine Learning Papers in Tier 1 Security Conferences},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623130},
doi = {10.1145/3576915.3623130},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3433–3459},
numpages = {27},
keywords = {machine learning, meta-science, reproducibility, security},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@article{vandewalle2009reproducible,
  title={Reproducible research in signal processing},
  author={Vandewalle, Patrick and Kovacevic, Jelena and Vetterli, Martin},
  journal={IEEE Signal Processing Magazine},
  volume={26},
  number={3},
  pages={37--47},
  year={2009},
  publisher={IEEE}
}

@article{seibold2021computational,
  title={A computational reproducibility study of PLOS ONE articles featuring longitudinal data analyses},
  author={Seibold, Heidi and Czerny, Severin and Decke, Siona and Dieterle, Roman and Eder, Thomas and Fohr, Steffen and Hahn, Nico and Hartmann, Rabea and Heindl, Christoph and Kopper, Philipp and others},
  journal={PLoS One},
  volume={16},
  number={6},
  pages={e0251194},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{hardwicke2020empirical,
  title={An empirical assessment of transparency and reproducibility-related research practices in the social sciences (2014--2017)},
  author={Hardwicke, Tom E and Wallach, Joshua D and Kidwell, Mallory C and Bendixen, Theiss and Cr{\"u}well, Sophia and Ioannidis, John PA},
  journal={Royal Society open science},
  volume={7},
  number={2},
  pages={190806},
  year={2020},
  publisher={The Royal Society}
}

@article{cervera2018try,
  title={Try to start it! the challenge of reusing code in robotics research},
  author={Cervera, Enric},
  journal={IEEE robotics and automation letters},
  volume={4},
  number={1},
  pages={49--56},
  year={2018},
  publisher={IEEE}
}

@article{riehl2025revisiting,
  title={Revisiting reproducibility in transportation simulation studies},
  author={Riehl, Kevin and Kouvelas, Anastasios and Makridis, Michail A},
  journal={European Transport Research Review},
  volume={17},
  number={1},
  pages={22},
  year={2025},
  publisher={Springer}
}

@article{siegelcore,
  title={CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark},
  author={Siegel, Zachary S and Kapoor, Sayash and Nadgir, Nitya and Stroebl, Benedikt and Narayanan, Arvind},
  journal={Transactions on Machine Learning Research},
  year={2024}
}

@inproceedings{huotala2024promise,
  title={The promise and challenges of using LLMs to accelerate the screening process of systematic reviews},
  author={Huotala, Aleksi and Kuutila, Miikka and Ralph, Paul and M{\"a}ntyl{\"a}, Mika},
  booktitle={Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
  pages={262--271},
  year={2024}
}

@article{mahmoudi2024critical,
  title={A critical assessment of large language models for systematic reviews: utilizing ChatGPT for complex data extraction},
  author={Mahmoudi, Hesam and Chang, Doris and Lee, Hannah and Ghaffarzadegan, Navid and Jalali, Mohammad S},
  journal={Available at SSRN 4797024},
  year={2024}
}

@article{lee2025role, title={The role of large language models in the peer-review process: opportunities and challenges for medical journal reviewers and editors}, author={Lee, Jisoo and Lee, Jieun and Yoo, Jeong-Ju}, journal={Journal of Educational Evaluation for Health Professions}, volume={22}, year={2025}, publisher={Korea Health Personnel Licensing Examination Institute} }

@article{goodman2016does,
  title={What does research reproducibility mean?},
  author={Goodman, Steven N and Fanelli, Daniele and Ioannidis, John PA},
  journal={Science translational medicine},
  volume={8},
  number={341},
  pages={341ps12--341ps12},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{son2024multi,
  title={Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?},
  author={Son, Guijin and Baek, SangWon and Nam, Sangdae and Jeong, Ilgyun and Kim, Seungone},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5606--5627},
  year={2024}
}

@article{gozzi2024comparative,
  title={Comparative Analysis of Prompt Strategies for Large Language Models: Single-Task vs. Multitask Prompts},
  author={Gozzi, Manuel and Di Maio, Federico},
  journal={Electronics},
  volume={13},
  number={23},
  pages={4712},
  year={2024},
  publisher={MDPI}
}

@article{peng2011reproducible,
  title={Reproducible research in computational science},
  author={Peng, Roger D},
  journal={Science},
  volume={334},
  number={6060},
  pages={1226--1227},
  year={2011},
  publisher={American Association for the Advancement of Science}
}

@article{stodden2014reproducible,
  title={The reproducible research movement in statistics},
  author={Stodden, Victoria},
  journal={Statistical Journal of the IAOS},
  volume={30},
  number={2},
  pages={91--93},
  year={2014},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{open2015estimating,
  title={Estimating the reproducibility of psychological science},
  author={Open Science Collaboration},
  journal={Science},
  volume={349},
  number={6251},
  pages={aac4716},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@misc{trc_guide2025,
  title        = {Guide for Authors - {Transportation Research Part C}: Emerging Technologies},
  howpublished = {\url{https://www.sciencedirect.com/journal/transportation-research-part-c-emerging-technologies/publish/guide-for-authors}},
  note         = {Accessed: 2025-06-16},
  year         = {2025}
}

@misc{trc_aimsscope2025,
  title        = {Aims \& Scope - {Transportation Research Part C}: Emerging Technologies},
  howpublished = {\url{https://www.sciencedirect.com/journal/transportation-research-part-c-emerging-technologies/about/aims-and-scope}},
  note         = {Accessed: 2025-06-16},
  year         = {2025}
}

@article{woelfle2011open,
  title={Open science is a research accelerator},
  author={Woelfle, Michael and Olliaro, Piero and Todd, Matthew H},
  journal={Nature chemistry},
  volume={3},
  number={10},
  pages={745--748},
  year={2011},
  publisher={Nature Publishing Group UK London}
}

@misc{unesco_openscience2022,
  author       = {{UNESCO}},
  title        = {Understanding Open Science},
  series       = {UNESCO Open Science Toolkit},
  volume       = {35},
  howpublished = {\url{https://doi.org/10.54677/UTCD9302}},
  year         = {2022},
  note         = {Document code: SC-PBS-STIP/2022/OST/1, 6 pages},
}


@book{national2019reproducibility,
  title={Reproducibility and replicability in science},
  author={National Academies of Sciences and Medicine and Policy and Global Affairs and Board on Research Data and Information and Division on Engineering and Physical Sciences and Committee on Applied and Theoretical Statistics and others},
  year={2019},
  publisher={National Academies Press}
}

@article{newell2002memoirs,
  title={Memoirs on highway traffic flow theory in the 1950s},
  author={Newell, Gordon F},
  journal={Operations Research},
  volume={50},
  number={1},
  pages={173--178},
  year={2002},
  publisher={INFORMS}
}

@article{nie2025brief,
  title={A brief history of travel forecasting},
  author={Nie, Marco},
  journal={Transportation},
  pages={1--26},
  year={2025},
  publisher={Springer}
}

@article{welch2019big,
  title={Big data in public transportation: a review of sources and methods},
  author={Welch, Timothy F and Widita, Alyas},
  journal={Transport reviews},
  volume={39},
  number={6},
  pages={795--818},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{wu_curseVariety2023,
  author       = {Cathy Wu and Daniel de Wolff},
  title        = {The curse of variety in transportation systems},
  journal      = {MIT News},
  year         = {2023},
  month        = aug,
  day          = {7},
  howpublished = {\url{https://news.mit.edu/2023/curse-variety-transporation-systems-cathy-wu-0807}},
  note         = {Accessed: 2025-06-16},
}


@article{sun2017discovering,
  title={Discovering themes and trends in transportation research using topic modeling},
  author={Sun, Lijun and Yin, Yafeng},
  journal={Transportation Research Part C: Emerging Technologies},
  volume={77},
  pages={49--66},
  year={2017},
  publisher={Elsevier}
}

@article{sun2020identifying,
  title={Identifying regional characteristics of transportation research with Transport Research International Documentation (TRID) data},
  author={Sun, Yanshuo and Kirtonia, Sajeeb},
  journal={Transportation Research Part A: Policy and Practice},
  volume={137},
  pages={111--130},
  year={2020},
  publisher={Elsevier}
}

@article{yang2020estimating,
  title={Estimating the deep replicability of scientific findings using human and artificial intelligence},
  author={Yang, Yang and Youyou, Wu and Uzzi, Brian},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={20},
  pages={10762--10768},
  year={2020},
  publisher={National Academy of Sciences}
}

@article{youyou2023discipline,
  title={A discipline-wide investigation of the replicability of Psychology papers over the past two decades},
  author={Youyou, Wu and Yang, Yang and Uzzi, Brian},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={6},
  pages={e2208863120},
  year={2023},
  publisher={National Academy of Sciences}
}

@article{stagge2019assessing,
  title={Assessing data availability and research reproducibility in hydrology and water resources},
  author={Stagge, James H and Rosenberg, David E and Abdallah, Adel M and Akbar, Hadia and Attallah, Nour A and James, Ryan},
  journal={Scientific data},
  volume={6},
  number={1},
  pages={1--12},
  year={2019},
  publisher={Nature Publishing Group}
}

@misc{Elsevier_JournalsDataXML_2024,
  author       = {{Elsevier}},
  title        = {{Journals Data XML}},
  year         = {2024},
  month        = nov,
  howpublished = {\url{https://supportcontent.elsevier.com/Support%20Hub/DaaS/Journals_Data_XML.pdf}},
  note         = {Data as a Service documentation. Accessed 17 Jun 2025}
}

@article{zbMATH02142783,
 author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
 title = {Latent {Dirichlet} allocation},
 fjournal = {Journal of Machine Learning Research (JMLR)},
 journal = {J. Mach. Learn. Res.},
 issn = {1532-4435},
 volume = {3},
 number = {4-5},
 pages = {993--1022},
 year = {2003},
 language = {English},
 doi = {10.1162/jmlr.2003.3.4-5.993},
 keywords = {68P20,68T50,68P05},
 zbMATH = {2142783},
 Zbl = {1112.68379}
}

@ARTICLE{Jelodar2019-bm,
  title     = "Latent Dirichlet allocation ({LDA}) and topic modeling: models,
               applications, a survey",
  author    = "Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and
               Jiang, Xiahui and Li, Yanchao and Zhao, Liang",
  journal   = "Multimed. Tools Appl.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  78,
  number    =  11,
  pages     = "15169--15211",
  month     =  jun,
  year      =  2019,
  language  = "en"
}

@techreport{Gemini2025,
  title={Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities},
  author={Gemini Team},
  year={2025},
  institution={Google},
  url={https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf},
  note={Accessed: 2025-06-20}
}


@article{Cervera2019TryTS,
  title={Try to Start It! The Challenge of Reusing Code in Robotics Research},
  author={Enric Cervera},
  journal={IEEE Robotics and Automation Letters},
  year={2019},
  volume={4},
  pages={49-56},
  url={https://api.semanticscholar.org/CorpusID:53440418}
}

@inbook{10.5555/3454287.3454779,
author = {Raff, Edward},
title = {A step toward quantifying independently reproducible machine learning research},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {492},
numpages = {11}
}

@ARTICLE{9732944,
  author={Raghupathi, Wullianallur and Raghupathi, Viju and Ren, Jie},
  journal={IEEE Access}, 
  title={Reproducibility in Computing Research: An Empirical Study}, 
  year={2022},
  volume={10},
  number={},
  pages={29207-29223},
  keywords={Reproducibility of results;Documentation;Computational modeling;Business;Codes;Testing;Information systems;Reproducibility;computing;method;data;experiment},
  doi={10.1109/ACCESS.2022.3158675}}

@article{PLOSReproducibility,
author = {Seibold, Heidi and Czerny, Severin and Bührmann, Siona and Dieterle, Roman and Eder, Thomas and Fohr, Steffen and Hahn, Nico and Hartmann, Rabea and Heindl, Christoph and Kopper, Philipp and Lepke, Dario and Loidl, Verena and Mandl, Maximilian and Musiol, Sarah and Peter, Jessica and Piehler, Alexander and Rojas, Elio and Schmid, Stefanie and Schmidt, Hannah and Nalenz, Malte},
year = {2021},
month = {06},
pages = {e0251194},
title = {A computational reproducibility study of PLOS ONE articles featuring longitudinal data analyses},
volume = {16},
journal = {PLOS ONE},
doi = {10.1371/journal.pone.0251194}
}


@article{
doi:10.1073/pnas.1708290115,
author = {Victoria Stodden  and Jennifer Seiler  and Zhaokun Ma },
title = {An empirical analysis of journal policy effectiveness for computational reproducibility},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {11},
pages = {2584-2589},
year = {2018},
doi = {10.1073/pnas.1708290115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1708290115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1708290115},
abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy—author remission of data and code postpublication upon request—an improvement over no policy, but currently insufficient for reproducibility.}}

@article{10.1371/journal.pbio.1002456,
    doi = {10.1371/journal.pbio.1002456},
    author = {Kidwell, Mallory C. AND Lazarević, Ljiljana B. AND Baranski, Erica AND Hardwicke, Tom E. AND Piechowski, Sarah AND Falkenberg, Lina-Sophia AND Kennett, Curtis AND Slowik, Agnieszka AND Sonnleitner, Carina AND Hess-Holden, Chelsey AND Errington, Timothy M. AND Fiedler, Susann AND Nosek, Brian A.},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency},
    year = {2016},
    month = {05},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pbio.1002456},
    pages = {1-15},
    number = {5},
}

@article{joubert2015repeatability,
  title={Repeatability \& reproducibility: Implications of using GPS data for freight activity chains},
  author={Joubert, Johan W and Meintjes, Sumarie},
  journal={Transportation Research Part B: Methodological},
  volume={76},
  pages={81--92},
  year={2015},
  publisher={Elsevier}
}

@article{barmpounakis2020new,
  title={On the new era of urban traffic monitoring with massive drone data: The pNEUMA large-scale field experiment},
  author={Barmpounakis, Emmanouil and Geroliminis, Nikolas},
  journal={Transportation research part C: emerging technologies},
  volume={111},
  pages={50--71},
  year={2020},
  publisher={Elsevier}
}

@article{gloudemans202324,
  title={I-24 motion: An instrument for freeway traffic science},
  author={Gloudemans, Derek and Wang, Yanbing and Ji, Junyi and Zachar, Gergely and Barbour, William and Hall, Eric and Cebelak, Meredith and Smith, Lee and Work, Daniel B},
  journal={Transportation Research Part C: Emerging Technologies},
  volume={155},
  pages={104311},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{seo2020evaluation,
  title={Evaluation of large-scale complete vehicle trajectories dataset on two kilometers highway segment for one hour duration: Zen Traffic Data},
  author={Seo, Toru and Tago, Yusuke and Shinkai, Norihito and Nakanishi, Masakazu and Tanabe, Jun and Ushirogochi, Daisuke and Kanamori, Shota and Abe, Atsushi and Kodama, Takashi and Yoshimura, Satoshi and others},
  booktitle={2020 International Symposium on Transportation Data and Modelling},
  year={2020}
}

@article{schicktanz2025dlr,
  title={The DLR Highway Traffic Dataset (DLR-HT): Longest Road User Trajectories on a German Highway},
  author={Schicktanz, Clemens and Klitzke, Lars and Gimm, Kay and L{\"u}dtke, Richard and Liesner, Karsten and Mosebach, Henning Hajo and Heuer, Fin and Wodtke, Axel and Asbach, Lennart},
  journal={Authorea Preprints},
  year={2025},
  publisher={Authorea}
}

@article{armeni2021towards,
  title={Towards wide-scale adoption of open science practices: The role of open science communities},
  author={Armeni, Kristijan and Brinkman, Loek and Carlsson, Rickard and Eerland, Anita and Fijten, Rianne and Fondberg, Robin and Heininga, Vera E and Heunis, Stephan and Koh, Wei Qi and Masselink, Maurits and others},
  journal={Science and Public Policy},
  volume={48},
  number={5},
  pages={605--611},
  year={2021},
  publisher={Oxford University Press UK}
}

@inproceedings{Olszewski2023reprosecurity,
author = {Olszewski, Daniel and Lu, Allison and Stillman, Carson and Warren, Kevin and Kitroser, Cole and Pascual, Alejandro and Ukirde, Divyajyoti and Butler, Kevin and Traynor, Patrick},
title = {"Get in Researchers; We're Measuring Reproducibility": A Reproducibility Study of Machine Learning Papers in Tier 1 Security Conferences},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623130},
doi = {10.1145/3576915.3623130},
abstract = {Reproducibility is crucial to the advancement of science; it strengthens confidence in seemingly contradictory results and expands the boundaries of known discoveries. Computer Security has the natural benefit of creating artifacts that should facilitate computational reproducibility, the ability for others to use someone else's code and data to independently recreate results, in a relatively straightforward fashion. While the Security community has recently increased its attention on reproducibility, an independent and comprehensive measurement of the current state of reproducibility has not been conducted. In this paper, we perform the first such study, targeting reproducible artifacts generated specifically by papers on machine learning security (one of the most popular areas in academic research) published in Tier 1 security conferences over the past ten years (2013-2022). We perform our measurement study of indirect and direct reproducibility over nearly 750 papers, their codebases, and datasets. Our analysis shows that there is no statistically significant difference between the availability of artifacts before and after the introduction of Artifact Evaluation Committees in Tier 1 conferences. However, based on three years of results, artifacts that pass through this process work at a higher rate than those that do not. From our collected findings, we offer data-driven suggestions for improving reproducibility in our community, including five common problems observed in our study. In so doing, we demonstrate that significant progress still needs to be made in computational reproducibility in Computer Security research.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3433–3459},
numpages = {27},
keywords = {machine learning, meta-science, reproducibility, security},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@misc{zhao2025sciarena,
      title={SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks}, 
      author={Yilun Zhao and Kaiyan Zhang and Tiansheng Hu and Sihong Wu and Ronan Le Bras and Taira Anderson and Jonathan Bragg and Joseph Chee Chang and Jesse Dodge and Matt Latzke and Yixin Liu and Charles McGrady and Xiangru Tang and Zihang Wang and Chen Zhao and Hannaneh Hajishirzi and Doug Downey and Arman Cohan},
      year={2025},
      eprint={2507.01001},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.01001}, 
}

@inproceedings{gundersen2025unreasonable,
  title={The unreasonable effectiveness of open science in ai: A replication study},
  author={Gundersen, Odd Erik and Cappelen, Odd and M{\o}ln{\aa}, Martin and Nilsen, Nicklas Grimstad},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={25},
  pages={26211--26219},
  year={2025}
}

@article{kapoor2023leakage,
  title        = {Leakage and the reproducibility crisis in ML-based science},
  author       = {Kapoor, Sayash and Narayanan, Arvind},
  journal      = {Patterns},
  volume       = {4},
  number       = {9},
  pages        = {100804},
  year         = {2023},
  publisher    = {Cell Press},
  doi          = {10.1016/j.patter.2023.100804},
  url          = {https://doi.org/10.1016/j.patter.2023.100804}
}

@misc{oxfordctl2025,
  author       = {{Centre for Teaching and Learning, University of Oxford}},
  title        = {Teaching Research Reproducibility Through AI-Human Collaboration},
  year         = {2025},
  howpublished = {\url{https://www.ctl.ox.ac.uk/reproducibility-of-research}},
  note         = {Accessed: 2025-09-12}
}

@article{springer_dh_reproducibility,
author = {Sean Grant and Sayak Khatua},
title = {Research transparency and reproducibility policies and programmes at the international initiative for impact evaluation},
journal = {Journal of Development Effectiveness},
volume = {16},
number = {3},
pages = {363--373},
year = {2024},
publisher = {Routledge},
doi = {10.1080/19439342.2024.2388102},
URL = {     
        https://doi.org/10.1080/19439342.2024.2388102    
},
eprint = { 
    
        https://doi.org/10.1080/19439342.2024.2388102
    
    

}

}

@article{han2025reproducible,
  author    = {Han, H.},
  title     = {Challenges of reproducible AI in biomedical data science},
  journal   = {BMC Medical Genomics},
  volume    = {18},
  number    = {Suppl 1},
  pages     = {8},
  year      = {2025},
  doi       = {10.1186/s12920-024-02072-6},
  url       = {https://doi.org/10.1186/s12920-024-02072-6}
}



@article{meijer2025proactive,
  author = {Meijer, Paul and Howard, Nicole and Liang, Jessica and Kelsey, Autumn and Subramanian, Sathya and Johnson, Ed and Mariz, Paul and Harvey, James and Ambrose, Madeline and Tereshchenko, Vitalii and Beaubien, Aldan and Inala, Neelima and Aggoune, Yousef and Pister, Stark and Vetto, Anne and Kinsey, Melissa and Bumol, Tom and Goldrath, Ananda and Li, Xiaojun and Torgerson, Troy and Skene, Peter and Okada, Lauren and La France, Christian and Thomson, Zach and Graybuck, Lucas},
  journal = {Royal Society Open Science},
  title = {Provide proactive reproducible analysis transparency with every publication},
  volume = {12},
  number = {3},
  pages = {241936},
  year = {2025},
  doi = {10.1098/rsos.241936},
  url = {https://doi.org/10.1098/rsos.241936}
}

@article{haibe-kains2020transparency,
  author = {Haibe-Kains, Benjamin and Adam, George A. and Hosny, Ahmed and others},
  title = {Transparency and reproducibility in artificial intelligence},
  journal = {Nature},
  volume = {586},
  pages = {E14--E16},
  year = {2020},
  doi = {10.1038/s41586-020-2766-y},
  url = {https://doi.org/10.1038/s41586-020-2766-y}
}

@inproceedings{bhaskar,
author = {Bhaskar, Adhithya and Stodden, Victoria},
title = {Reproscreener: Leveraging LLMs for Assessing Computational Reproducibility of Machine Learning Pipelines},
year = {2024},
isbn = {9798400705304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641525.3663629},
doi = {10.1145/3641525.3663629},
abstract = {The increasing reliance on machine learning models in scientific research and day-to-day applications – and the near-opacity of their associated computational methods – creates a widely recognized need to enable others to verify results coming from Machine Learning Pipelines. In this work we use an empirical approach to build on efforts to define and deploy structured publication standards that allow machine learning research to be automatically assessed and verified, enabling greater reliability and trust in results. To automate the assessment of a set of publication standards for Machine Learning Pipelines we developed Reproscreener; a novel, open-source software tool (see https://reproscreener.org/). We benchmark Reproscreener’s automatic reproducibility assessment against a novel manually labeled “gold standard” dataset of machine learning arXiv preprints. Our empirical evaluation has a dual goal: to assess Reproscreener’s performance; and to uncover gaps and opportunities in current reproducibility standards. We develop reproducibility assessment metrics we called the Repo Metrics to provide a novel overall assessment of the re-executability potential of the Machine Learning Pipeline, called the ReproScore. We used two approaches to the automatic identification of reproducibility metrics, keywords and LLM tools, and found the reproducibility metric evaluation performance of Large Language Model (LLM) tools superior to keyword associations.},
booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability},
pages = {101–109},
numpages = {9},
keywords = {Computational Reproducibility, CyberInfrastructure, Machine Learning, Open Code, Open Data, ReproScore, Reproducibility Policy, Reproscreener},
location = {Rennes, France},
series = {ACM REP '24}
}

@article{BRINCKMAN2019854,
title = {Computing environments for reproducibility: Capturing the “Whole Tale”},
journal = {Future Generation Computer Systems},
volume = {94},
pages = {854-867},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17310695},
author = {Adam Brinckman and Kyle Chard and Niall Gaffney and Mihael Hategan and Matthew B. Jones and Kacper Kowalik and Sivakumar Kulasekaran and Bertram Ludäscher and Bryce D. Mecum and Jarek Nabrzyski and Victoria Stodden and Ian J. Taylor and Matthew J. Turk and Kandace Turner},
keywords = {Living publications, Reproducibility, Provenance, Data sharing, Code sharing},
abstract = {The act of sharing scientific knowledge is rapidly evolving away from traditional articles and presentations to the delivery of executable objects that integrate the data and computational details (e.g., scripts and workflows) upon which the findings rely. This envisioned coupling of data and process is essential to advancing science but faces technical and institutional barriers. The Whole Tale project aims to address these barriers by connecting computational, data-intensive research efforts with the larger research process—transforming the knowledge discovery and dissemination process into one where data products are united with research articles to create “living publications” or tales. The Whole Tale focuses on the full spectrum of science, empowering users in the long tail of science, and power users with demands for access to big data and compute resources. We report here on the design, architecture, and implementation of the Whole Tale environment.}
}

@inproceedings{MERIT,
author = {Wonsil, Joseph and Sullivan, Jack and Seltzer, Margo and Pocock, Adam},
title = {Integrated Reproducibility with Self-describing Machine Learning Models},
year = {2023},
isbn = {9798400701764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589806.3600039},
doi = {10.1145/3589806.3600039},
abstract = {Researchers and data scientists frequently want to collaborate on machine learning models. However, in the presence of sharing and simultaneous experimentation, it is challenging both to determine if two models were trained identically and to reproduce precisely someone else’s training process. We demonstrate how provenance collection that is tightly integrated into a machine learning library facilitates reproducibility. We present MERIT, a reproducibility system that leverages a robust configuration system and extensive provenance collection to exactly reproduce models, given only a model object. We integrate MERIT with Tribuo, an open-source Java-based machine learning library. Key features of this integrated reproducibility framework include controlling for sources of non-determinism in a multi-threaded environment and exposing the training differences between two models in a human-readable form. Our system allows simple reproduction of deployed Tribuo models without any additional information, ensuring data science research is reproducible. Our framework is open-source and available under an Apache 2.0 license.},
booktitle = {Proceedings of the 2023 ACM Conference on Reproducibility and Replicability},
pages = {1–14},
numpages = {14},
keywords = {Machine Learning, provenance, reproducibility},
location = {Santa Cruz, CA, USA},
series = {ACM REP '23}
}


@article{DeBlanc2020,
  author    = {DeBlanc, J. and Kay, B. and Lehrich, J. and Kamdar, N. and Valley, T. S. and Ayanian, J. Z. and Nallamothu, B. K.},
  title     = {Availability of Statistical Code From Studies Using Medicare Data in General Medical Journals},
  journal   = {JAMA Internal Medicine},
  volume    = {180},
  number    = {6},
  pages     = {905--907},
  year      = {2020},
  month     = {June},
  doi       = {10.1001/jamainternmed.2020.0671},
  pmid      = {32282018},
  pmcid     = {PMC7154950}
}

@article{gunzer2022reproducibility,
  title={Reproducibility of artificial intelligence models in computed tomography of the head: a quantitative analysis},
  author={Gunzer, F. and Jantscher, M. and Hassler, E.M. and Kau, T. and Reishofer, G. and others},
  journal={Insights into Imaging},
  volume={13},
  number={1},
  pages={173},
  year={2022},
  publisher={Springer},
  doi={10.1186/s13244-022-01311-7}
}

@misc{dobbins2025largelanguagemodelbasedagents,
      title={Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease}, 
      author={Nic Dobbins and Christelle Xiong and Kristine Lan and Meliha Yetisgen},
      year={2025},
      eprint={2505.23852},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.23852}, 
}

@misc{xiang2025scireplicatebenchbenchmarkingllmsagentdriven,
      title={SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers}, 
      author={Yanzheng Xiang and Hanqi Yan and Shuyin Ouyang and Lin Gui and Yulan He},
      year={2025},
      eprint={2504.00255},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00255}, 
}

@misc{hu2025reprobench,
  title={REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?},
  author={Chuxuan Hu and Liyun Zhang and Yeji Lim and Aum Wadhwani and Austin Peters and Daniel Kang},
  year={2025},
  eprint={2507.18901},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  note={arXiv:2507.18901v1 [cs.CL], 25 Jul 2025}
}
